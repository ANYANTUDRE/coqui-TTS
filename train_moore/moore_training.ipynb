{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b146376364ec4850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:17:06.871953Z",
     "start_time": "2024-06-14T11:17:05.887182Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets, Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "#import torchaudio\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import gc\n",
    "import soundfile as sf\n",
    "import IPython\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "#import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb152d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip uninstall -y torch\\n!pip install torch==2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!pip uninstall -y torch\n",
    "!pip install torch==2.4.1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "763526d46c279f9b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are logged in to Hugging Face now!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Access the token key\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "def login_hugging_face(token: str) -> None:\n",
    "    \"\"\"\n",
    "    Loging to Hugging Face portal with a given token.\n",
    "    \"\"\"\n",
    "    api = HfApi(token=token)\n",
    "    #api.set_access_token(token)\n",
    "    #folder = HfFolder()\n",
    "    #folder.save_token(token)\n",
    "    return None\n",
    "\n",
    "login_hugging_face(HF_TOKEN)\n",
    "print('We are logged in to Hugging Face now!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c910258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['fr', 'mos', 'audio', 'speaker_id', 'index', 'denoised_audio', 'enhanced_audio'],\n",
      "        num_rows: 1657\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset splits\n",
    "dataset = load_dataset(\"ArissBandoss/sentences-audio-texte-denoised-enhanced\")\n",
    "\n",
    "# Get the train and test datasets\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Concatenate the datasets\n",
    "dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "\n",
    "# Create a new DatasetDict with a single 'train' key\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset\n",
    "})\n",
    "\n",
    "# Check the structure of the new DatasetDict\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788f7784839b3d8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:17:16.381971Z",
     "start_time": "2024-06-14T11:17:12.359727Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['fr', 'text', 'audio', 'speaker_id', 'index', 'denoised_audio', 'enhanced_audio', 'lang'],\n",
       "        num_rows: 1657\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].add_column('lang', ['mos'] * len(dataset['train'])).rename_column(\"mos\", \"text\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eba6345f8677b80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:17:22.302446Z",
     "start_time": "2024-06-14T11:17:22.298312Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHAR_LIMIT = {\n",
    "    \"en\": 250,\n",
    "    \"fr\": 273,\n",
    "    \"es\": 239,\n",
    "    \"it\": 213,\n",
    "    \"mos\": 300,\n",
    "}\n",
    "\n",
    "def iterable_to_dataset(iterable_dataset, lang, num_rows):\n",
    "    \"\"\"\n",
    "    Converts an IterableDataset to a Dataset with a specified number of rows using a while loop.\n",
    "\n",
    "    Parameters:\n",
    "    - iterable_dataset (IterableDataset): The input IterableDataset from Hugging Face datasets.\n",
    "    - num_rows (int): The number of rows desired in the output Dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dataset: A Dataset object with the specified number of rows.\n",
    "    \"\"\"\n",
    "    # Create an iterator from the iterable dataset\n",
    "    iterator = iter(iterable_dataset)\n",
    "\n",
    "    # Initialize an empty list to store the dataset rows\n",
    "    rows = []\n",
    "\n",
    "    # Initialize the tqdm progress bar\n",
    "    progress_bar = tqdm(total=num_rows, desc='Converting', unit='row')\n",
    "\n",
    "    try:\n",
    "        # Collect the specified number of rows\n",
    "        while len(rows) < num_rows:\n",
    "            item = next(iterator)\n",
    "            if len(item['text']) <= CHAR_LIMIT[lang]:\n",
    "                item[\"lang\"] = lang\n",
    "                rows.append(item)\n",
    "                progress_bar.update(1)\n",
    "    except StopIteration:\n",
    "        # End of iterator reached\n",
    "        print(\"End of iterable dataset reached before requested number of rows.\")\n",
    "    finally:\n",
    "        progress_bar.close()\n",
    "\n",
    "    # Convert the list of rows to a Dataset object\n",
    "    converted_dataset = Dataset.from_pandas(pd.DataFrame(rows))\n",
    "\n",
    "    return converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e79663c64d5a8f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:17:41.314624Z",
     "start_time": "2024-06-14T11:17:41.309666Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_audio_file(example, audio_column, output_dir, index):\n",
    "    \"\"\"\n",
    "    Creates a single audio file from the 'audio' column of an example and returns the file path.\n",
    "    \"\"\"\n",
    "    # Construct the output file path\n",
    "    audio_filename = f\"audio_{index}.wav\"\n",
    "    audio_filepath = os.path.join(output_dir, audio_filename)\n",
    "\n",
    "    # If file does not exist, write the audio data to the file\n",
    "    if not os.path.isfile(audio_filepath):\n",
    "        # Extract audio data and sample rate from the example\n",
    "        audio_data = example[audio_column]['array']\n",
    "        sample_rate = example[audio_column]['sampling_rate']\n",
    "\n",
    "        # Save the audio file\n",
    "        sf.write(audio_filepath, audio_data, sample_rate)\n",
    "\n",
    "    return {\"audio_file_path\": audio_filepath}\n",
    "\n",
    "\n",
    "\n",
    "def batch_create_audio_files_and_update_dataset(dataset, audio_column, output_dir):\n",
    "    \"\"\"\n",
    "    Maps over the dataset, creates audio files and updates the dataset with the file paths.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Use the .map() function to process the dataset and create audio files\n",
    "    dataset_with_audio_paths = dataset.map(\n",
    "        lambda example, idx: create_audio_file(example, audio_column, output_dir, idx),\n",
    "        with_indices=True,  # Pass example indices to the map function\n",
    "        num_proc=12\n",
    "    )\n",
    "\n",
    "    return dataset_with_audio_paths\n",
    "\n",
    "\n",
    "\n",
    "def create_audio_files_and_update_dataset(dataset, audio_column, output_dir):\n",
    "    \"\"\"\n",
    "    Create audio files from the 'audio' column of a Hugging Face dataset and update the dataset with file paths.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: The input dataset that contains the 'audio' column.\n",
    "    - audio_column: The name of the column containing the audio data (datasets.Audio feature).\n",
    "    - output_dir: The directory where audio files will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - The updated dataset with the 'audio' column containing the file paths of saved audio files.\n",
    "    \"\"\"\n",
    "    # Make sure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare a list to hold the file paths, to avoid modifying the dataset in-place\n",
    "    audio_file_paths = []\n",
    "\n",
    "    for index, example in tqdm(enumerate(dataset), total=len(dataset), desc=\"Creating audio files\", unit=\"file\"):\n",
    "        audio_filename = f\"audio_{index}.wav\"\n",
    "        audio_filepath = os.path.join(output_dir, audio_filename)\n",
    "\n",
    "        if os.path.isfile(audio_filepath):\n",
    "            audio_file_paths.append(audio_filepath)\n",
    "            continue\n",
    "\n",
    "        audio_data = example[audio_column]['array']\n",
    "        # Typically, the sample rate should also be retrieved from the dataset\n",
    "        sample_rate = example[audio_column]['sampling_rate']\n",
    "\n",
    "        # Save the audio file\n",
    "        sf.write(audio_filepath, audio_data, sample_rate)\n",
    "\n",
    "        # Append the file path to the list\n",
    "        audio_file_paths.append(audio_filepath)\n",
    "\n",
    "        # Option to clear memory if needed, uncomment if large arrays are involved\n",
    "        del audio_data\n",
    "        gc.collect()\n",
    "\n",
    "    # Update the dataset with the new file paths\n",
    "    dataset = dataset.add_column(\"audio_file_path\", audio_file_paths)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Function to create the metadata file\n",
    "def create_metadata_file(dataset, output_dir='MyTTSDataSet', filename='metadata.txt'):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Define the path to the metadata file\n",
    "    metadata_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Open the metadata file in write mode\n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        # Iterate over each item in the dataset\n",
    "        for item in dataset:\n",
    "            # Your dataset should have an 'audio' column with a dictionary containing the file path and 'array' for the audio data\n",
    "            audio_path = item['audio_file_path'].replace(\".wav\", \"\")\n",
    "            text = item['text'].replace(\" \", \" \").replace(\" \", \" \").replace(\"\\n\", \" \")\n",
    "            normalized_text = text\n",
    "            speaker_id = item['speaker_id']\n",
    "            lang = item['lang']\n",
    "\n",
    "            # Write the formatted data to the metadata file\n",
    "            f.write(f\"{audio_path}|{text}|{normalized_text}|{speaker_id}|{lang}\\n\")\n",
    "\n",
    "    return metadata_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "857e0e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7edc83c8f56f0f3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:18:47.980674Z",
     "start_time": "2024-06-14T11:18:39.041036Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb000828f01c4d04bb78d4025c5c937f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/1657 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = batch_create_audio_files_and_update_dataset(\n",
    "    dataset,\n",
    "    audio_column=\"denoised_audio\",\n",
    "    output_dir=\"/teamspace/studios/this_studio/coqui-TTS/train_moore/dataset/audios/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8601b6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['fr', 'text', 'audio', 'speaker_id', 'index', 'denoised_audio', 'enhanced_audio', 'lang', 'audio_file_path'],\n",
       "        num_rows: 1657\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da6362b069800e51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:18:58.562483Z",
     "start_time": "2024-06-14T11:18:58.556098Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['fr', 'text', 'audio', 'speaker_id', 'index', 'denoised_audio', 'enhanced_audio', 'lang', 'audio_file_path'],\n",
       "        num_rows: 1408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['fr', 'text', 'audio', 'speaker_id', 'index', 'denoised_audio', 'enhanced_audio', 'lang', 'audio_file_path'],\n",
       "        num_rows: 249\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = dataset[\"train\"].train_test_split(test_size=0.15, seed=2024)\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb8e1376e5babb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:19:02.545795Z",
     "start_time": "2024-06-14T11:19:01.980039Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/coqui-TTS/train_moore/dataset/metadata_val.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"/teamspace/studios/this_studio/coqui-TTS/train_moore/dataset/\"\n",
    "\n",
    "create_metadata_file(dataset_dict['train'], output_dir=dataset_path, filename='metadata.txt')\n",
    "create_metadata_file(dataset_dict['test'],  output_dir=dataset_path, filename='metadata_val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "334ca97cc30229a5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/coqui-TTS/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      " > Loading checkpoint with 1552 additional tokens.\n",
      "/teamspace/studios/this_studio/coqui-TTS/TTS/tts/layers/tortoise/arch_utils.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.mel_norms = torch.load(f)\n",
      "/teamspace/studios/this_studio/coqui-TTS/TTS/tts/layers/xtts/trainer/gpt_trainer.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dvae_checkpoint = torch.load(self.args.dvae_checkpoint, map_location=torch.device(\"cpu\"))\n",
      ">> DVAE weights restored from: /teamspace/studios/this_studio/coqui-TTS/train_moore/run/training/CHECKPOINT_GPT_XTTS_v2.0_MOS_FT_2/dvae.pth\n",
      "108it [00:00, 66322.82it/s]\n",
      " | > Found 108 files in /teamspace/studios/this_studio/coqui-TTS/train_moore/dataset\n",
      "12it [00:00, 22036.62it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Num. of CPUs: 4\n",
      " | > Num. of Torch Threads: 1\n",
      " | > Torch seed: 1\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=/teamspace/studios/this_studio/coqui-TTS/train_moore/run/training/RUN_GPT_XTTS_v2.0_MOS_FT_2-September-18-2024_02+49PM-0000000\n",
      "\n",
      " > Model has 521622095 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/15\u001b[0m\n",
      " --> /teamspace/studios/this_studio/coqui-TTS/train_moore/run/training/RUN_GPT_XTTS_v2.0_MOS_FT_2-September-18-2024_02+49PM-0000000\n",
      " > Sampling by language: dict_keys([''])\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\n",
      "\u001b[1m > TRAINING (2024-09-18 14:49:45) \u001b[0m\n",
      "DEBUGGINGGGGGGG LENGHT ====>>> 3\n"
     ]
    }
   ],
   "source": [
    "#! CUDA_VISIBLE_DEVICES=\"0\" python /teamspace/studios/this_studio/coqui-TTS/train_moore/train_gpt_xtts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0cd98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
